---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &app ollama
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: ollama
      version: 1.2.0
      sourceRef:
        kind: HelmRepository
        name: ollama
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    ollama:
      gpu:
        # -- Enable GPU integration
        enabled: true
        # -- GPU type: 'nvidia' or 'amd'
        type: 'nvidia'
        # -- Specify the number of GPU
        number: 1
      # -- List of models to pull at container startup
      models:
        pull:
          - codellama
          - mistral
          - llama3

    extraEnv:
      - name: OLLAMA_DEBUG
        value: "1"

    ingress:
      enabled: true
      className: internal
      hosts:
        - host: ollama.oxygn.dev
          paths:
            - path: /
              pathType: Prefix

    persistentVolume:
      enabled: true
      existingClaim: ollama

    resources:
      requests:
        cpu: 200m
        memory: 1Gi
      limits:
        nvidia.com/gpu: 1
        memory: 6Gi

    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: nvidia.com/gpu.present
                  operator: In
                  values:
                    - "true"

    nodeSelector:
      nvidia.com/gpu.present: "true"

    runtimeClassName: nvidia
